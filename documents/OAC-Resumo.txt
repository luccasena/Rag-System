Resumo Detalhado de Arquitetura e Organização de Computadores
Este documento apresenta um resumo detalhado dos conceitos fundamentais e avançados em arquitetura e organização de computadores, baseado na 10ª edição do livro de William Stallings.
Parte I: Introdução
1. Conceitos Fundamentais e Evolução do Computador
Para compreender a arquitetura e a organização de computadores, é essencial dominar alguns conceitos básicos que formam o alicerce de toda a computação moderna.
Hardware e Software
A divisão mais fundamental em um sistema computacional é entre hardware e software.
Hardware: Refere-se a todos os componentes físicos e tangíveis do computador. Inclui a Unidade Central de Processamento (CPU), a memória (RAM, caches), os dispositivos de armazenamento (SSDs, HDDs), os periféricos de entrada e saída (teclado, monitor, mouse) e todos os circuitos e fios que os conectam. É a "máquina" em si.
Software: É o conjunto de instruções, dados ou programas usados para operar computadores e executar tarefas específicas. O software direciona o hardware. Divide-se em duas categorias principais:
Software de Sistema: Gerencia o hardware e fornece a plataforma para a execução de outros softwares. O exemplo mais importante é o Sistema Operacional (SO), que controla todos os recursos do computador.
Software de Aplicação: Programas projetados para que os usuários finais realizem tarefas específicas, como navegadores web, processadores de texto, jogos e planilhas.
A relação entre eles é simbiótica: o software não pode ser executado sem o hardware para executar as instruções, e o hardware é inútil sem o software para lhe dizer o que fazer.
Organização vs. Arquitetura:
Arquitetura: Refere-se aos atributos de um sistema que são visíveis a um programador de linguagem de máquina. Inclui o conjunto de instruções, tipos de dados, mecanismos de E/S e modos de endereçamento. É o "o quê" o computador faz. Por exemplo, a arquitetura define se existe uma instrução de multiplicação.
Organização: Refere-se a como os atributos da arquitetura são implementados. Inclui os detalhes de hardware transparentes para o programador, como sinais de controle, interfaces e a tecnologia de memória utilizada. É o "como" o computador faz. Por exemplo, a organização define se a instrução de multiplicação é implementada por uma unidade de hardware dedicada ou por meio de adições repetidas na ALU.
Estrutura e Função:
Função: Um computador executa quatro funções básicas: processamento de dados (operações lógicas e aritméticas), armazenamento de dados (memória), movimentação de dados (E/S) e controle (gerenciado pela unidade de controle).
Estrutura: Os componentes principais são a Unidade Central de Processamento (CPU), a memória principal e os módulos de Entrada/Saída (E/S), conectados por uma estrutura de interconexão (ex: barramento). A CPU, por sua vez, é estruturada em uma ALU, uma unidade de controle e registradores.
Camadas de Abstração e Hierarquia
O projeto de computadores complexos só é possível através do uso de abstração. Em cada nível do sistema, os detalhes complexos do nível inferior são ocultados, permitindo que projetistas e programadores trabalhem com uma visão simplificada. Isso cria uma hierarquia de camadas, desde os transistores físicos no nível mais baixo, passando pela lógica digital, microarquitetura, sistema operacional, e culminando nos softwares de aplicação no topo. A hierarquia de memória (registradores, cache, memória principal, armazenamento secundário) é um exemplo prático desse princípio, balanceando velocidade, custo e capacidade.
Evolução Histórica
1ª Geração (1940-1950, Válvulas): Caracterizada pelo uso de válvulas a vácuo, que eram grandes, consumiam muita energia e eram pouco confiáveis. Um dos primeiros e mais famosos computadores desta era foi o ENIAC (Electronic Numerical Integrator and Computer). Embora revolucionário por sua velocidade, o ENIAC não possuía um programa armazenado; sua programação era um processo físico e árduo, que envolvia a reconfiguração manual de cabos e interruptores para cada nova tarefa.
A mudança de paradigma veio com o matemático John von Neumann e seu conceito de programa armazenado. A arquitetura de von Neumann, como ficou conhecida, propunha que tanto as instruções do programa quanto os dados a serem processados residissem juntos na mesma memória de leitura-escrita. Essa foi uma ideia transformadora, pois permitiu que os computadores fossem reprogramados de forma rápida e flexível, apenas alterando o conteúdo da memória, sem necessidade de alterações físicas no hardware.
O computador IAS, construído no Instituto de Estudos Avançados de Princeton, é o protótipo que materializou essa arquitetura. Ele continha os principais componentes do modelo de von Neumann: uma memória de 4.096 palavras para dados e instruções, uma Unidade Lógica e Aritmética (ALU), uma Unidade de Controle para orquestrar as operações, e mecanismos de E/S. O funcionamento era governado por um ciclo de instrução (busca e execução) gerenciado por registradores essenciais como o MBR, MAR e o contador de programa (PC), estabelecendo o modelo fundamental para os computadores que se seguiram.
2ª Geração (1950-1960, Transistores): A substituição de válvulas por transistores resultou em computadores drasticamente menores, mais baratos, mais rápidos e mais confiáveis. O IBM 7094 é um exemplo representativo, que introduziu inovações como canais de dados (módulos de E/S independentes com seus próprios processadores) e multiplexadores para gerenciar o acesso à memória, aliviando a carga da CPU.
3ª Geração (1960-1970, Circuitos Integrados - CI): O uso de CIs (circuitos integrados) permitiu a integração de múltiplos transistores em um único chip de silício. Isso viabilizou a criação de "famílias" de computadores, como o IBM System/360, com a mesma arquitetura, mas diferentes implementações, permitindo que os clientes atualizassem o hardware sem perder o investimento em software. Outro marco foi o DEC PDP-8, um minicomputador que popularizou a estrutura de barramento, hoje universal em microcomputadores.
Gerações Posteriores (LSI, VLSI, ULSI): A integração em larga escala continuou, permitindo a criação de memórias semicondutoras (substituindo as de núcleo magnético) e microprocessadores (uma CPU inteira em um único chip), dando origem a arquiteturas poderosas como a família Intel x86 e a ARM, que dominam a computação até hoje.
2. Questões de Desempenho
Projeto para Desempenho: A otimização do desempenho envolve um esforço contínuo para aumentar a taxa de execução de instruções. As principais técnicas incluem:
Pipeline: Sobrepõe a execução de múltiplas instruções em diferentes estágios, como uma linha de montagem.
Previsão de Desvio: Tenta adivinhar o resultado de desvios condicionais para evitar que o pipeline pare e precise ser recarregado (uma condição conhecida como "bolha no pipeline").
Execução Superescalar: Utiliza múltiplas unidades de execução para processar várias instruções completamente em paralelo. Um desafio central é o balanço de desempenho: a velocidade do processor aumentou muito mais rápido que a da memória e da E/S. Para evitar que a CPU fique ociosa esperando por dados, são usadas hierarquias de memória complexas (caches) e barramentos de alta velocidade.
Leis Fundamentais:
Lei de Amdahl: Descreve o ganho de desempenho (speedup) máximo que pode ser obtido ao paralelizar uma porção de um programa. Ela demonstra que a parte do código que permanece inerentemente sequencial impõe um limite rígido ao ganho total, não importando quantos processadores sejam adicionados.
Lei de Little: Um princípio fundamental da teoria de filas, muito útil para análise de desempenho de sistemas. Ela relaciona o número médio de itens em um sistema (L), a taxa média de chegada de itens (λ) e o tempo médio que um item passa no sistema (W) através da simples fórmula: L = λW.
Benchmarks: Medidas de desempenho bruto como MIPS e MFLOPS são enganosas, pois não refletem a complexidade das instruções ou a carga de trabalho do mundo real. Por isso, a indústria utiliza benchmarks padronizados, como os da suíte SPEC (Standard Performance Evaluation Corporation). Esses benchmarks executam um conjunto de programas reais (compiladores, aplicações científicas, etc.) para fornecer uma avaliação de desempenho mais precisa e justa, permitindo uma comparação significativa entre diferentes arquiteturas.
Parte II: O Sistema de Computação
3. Visão de Alto Nível da Função e Interconexão
Ciclo de Instrução: A função básica do computador é executar programas. O ciclo de instrução é o processo repetitivo de: 1) Buscar uma instrução da memória; 2) Decodificá-la para determinar a operação e os operandos; e 3) Executá-la.
Interrupções: São um mecanismo crucial para a eficiência. Em vez de a CPU gastar tempo em um loop de espera ("polling") por uma operação de E/S lenta, o módulo de E/S pode enviar um sinal de interrupção à CPU quando a operação estiver concluída. A CPU então suspende sua tarefa atual, trata a interrupção e retoma seu trabalho, evitando o desperdício de ciclos.
Estruturas de Interconexão: Conectam os módulos principais (CPU, memória, E/S). O barramento (uma via de comunicação compartilhada) foi a estrutura dominante por muito tempo. No entanto, à medida que a velocidade aumentava, a contenção pelo barramento se tornou um gargalo. Sistemas modernos usam interconexões ponto a ponto, como QuickPath Interconnect (QPI) da Intel e PCI Express (PCIe), que fornecem conexões diretas entre os componentes, resultando em menor latência e maior taxa de transferência de dados.
4. Memória Cache
Princípios e Localidade: A cache é uma memória pequena e extremamente rápida (SRAM) localizada entre a CPU e a memória principal (DRAM). Ela explora o princípio da localidade de referência:
Localidade Temporal: Se um item é referenciado, é provável que ele seja referenciado novamente em breve.
Localidade Espacial: Se um item é referenciado, é provável que itens em posições de memória próximas também sejam referenciados. Ao armazenar cópias de dados e instruções usados recentemente, a cache reduz drasticamente o tempo médio de acesso à memória.
Projeto de Cache:
Função de Mapeamento: Determina onde um bloco de memória pode ser colocado na cache. As técnicas são: direto (mapeamento simples e rápido, mas pode levar a conflitos), associativo (flexível, mas com hardware de comparação caro) e associativo por conjunto (um compromisso entre os dois).
Algoritmo de Substituição: Quando a cache está cheia, decide qual bloco descartar. O mais comum é o LRU (Least Recently Used).
Política de Escrita: Define como as escritas em dados na cache são propagadas para a memória principal. Write-through é mais simples e mantém a consistência, mas gera mais tráfego de barramento. Write-back é mais eficiente em termos de desempenho, mas requer mecanismos mais complexos para garantir a coerência.
Níveis de Cache: Sistemas modernos usam uma hierarquia com múltiplos níveis (L1, L2, L3). A L1 é a menor e mais rápida, dedicada a um único core, enquanto a L2 e a L3 são progressivamente maiores, mais lentas e frequentemente compartilhadas entre múltiplos cores.
5. Memória Interna
Tecnologia de Memória Semicondutora
Em computadores mais antigos, a memória principal era comumente feita de anéis ferromagnéticos, conhecidos como "cores", termo que por vezes ainda é usado. Com a microeletrônica, a memória de núcleo magnético foi superada pelos chips semicondutores, que são hoje quase universais para a memória principal.
Organização da Célula de Memória:
O elemento básico de uma memória semicondutora é a célula de memória. Todas as células semicondutoras compartilham três propriedades fundamentais:
Apresentam dois estados estáveis (ou semiestáveis), que representam os valores binários 0 e 1.
São capazes de serem gravadas para definir o estado.
São capazes de serem lidas para verificar o estado.
Uma célula de memória típica possui três terminais:
Terminal de Seleção: Ativa a célula para uma operação de leitura ou escrita.
Terminal de Controle: Indica se a operação é de leitura ou de escrita.
Terminal de Dados/Sentido: Para a escrita, fornece o sinal elétrico que define o estado (0 ou 1). Para a leitura, é usado para a saída do estado da célula.
DRAM e SRAM
As memórias semicondutoras são de acesso aleatório (RAM - Random Access Memory), significando que palavras individuais são acessadas diretamente pela lógica de endereçamento. A RAM se caracteriza pela capacidade de ler e escrever dados de forma rápida e por ser volátil (perde os dados sem energia).
RAM Dinâmica (DRAM):
Tecnologia: Armazena dados como carga elétrica em capacitores. A presença ou ausência de carga representa 1 ou 0.
Operação: Como os capacitores descarregam naturalmente, a DRAM exige atualização (refresh) periódica para manter os dados, daí o nome "dinâmica".
Estrutura: Uma célula de DRAM é muito simples e pequena, consistindo basicamente em um capacitor e um transistor, o que permite alta densidade (mais bits por área) e menor custo.
Uso: Devido à sua densidade e custo, é a tecnologia preferida para a memória principal do computador.
RAM Estática (SRAM):
Tecnologia: Armazena dados usando flip-flops, a mesma lógica de portas encontrada nos processadores.
Operação: A SRAM mantém os dados enquanto houver energia, sem necessidade de atualização.
Estrutura: Uma célula de SRAM é mais complexa, maior e mais cara que uma de DRAM.
Desempenho: É significativamente mais rápida que a DRAM.
Uso: Devido à sua alta velocidade e custo, a SRAM é a tecnologia usada para construir as memórias cache.
Evoluções da DRAM: SDRAM e DDR-SDRAM
A interface com a memória principal é um dos gargalos mais críticos do sistema. Para além da criação de caches com SRAM, a própria arquitetura da DRAM evoluiu para aumentar o desempenho.
DRAM Síncrona (SDRAM): Ao contrário da DRAM tradicional (assíncrona), a SDRAM opera de forma síncrona com o clock do sistema. Isso elimina estados de espera, pois a DRAM e o processador operam na mesma velocidade do barramento de memória. O processador envia uma instrução e um endereço, e a SDRAM responde após um número pré-definido de ciclos de clock, permitindo que o processador execute outras tarefas nesse meio-tempo. A SDRAM também introduziu o modo de rajada (burst mode), que permite a transferência rápida de uma sequência de bits de dados após o primeiro acesso, e uma arquitetura de múltiplos bancos para aumentar o paralelismo.
DDR-SDRAM (Double-Data-Rate SDRAM): Esta evolução da SDRAM aumenta ainda mais as taxas de dados de três maneiras principais:
Taxa Dupla de Dados: A transferência de dados ocorre tanto na borda de subida quanto na de descida do clock, dobrando a taxa de transferência efetiva.
Frequência Maior: Utiliza frequências de clock mais altas no barramento.
Buffer de Pré-Busca (Prefetch): Um buffer no chip da DRAM pré-carrega os dados que serão colocados no barramento. A cada nova geração (DDR, DDR2, DDR3), o tamanho desse buffer aumentou (2 bits, 4 bits, 8 bits, respectivamente), permitindo que a interface de E/S, mais rápida, seja alimentada com dados em paralelo pelo núcleo mais lento da DRAM. A DDR4 inovou ao introduzir grupos de bancos, permitindo operações paralelas em diferentes bancos como se o buffer de pré-busca fosse maior, aumentando ainda mais o desempenho.
Tipos de ROM
A Memória Somente de Leitura (ROM) é não volátil (mantém os dados sem energia) e contém um padrão de dados permanente. É usada para armazenar firmware, como sub-rotinas de biblioteca ou programas de sistema.
ROM: Dados gravados durante a fabricação. Inviável para pequenas quantidades devido ao custo fixo e à impossibilidade de corrigir erros.
PROM (Programmable ROM): Pode ser gravada eletricamente uma única vez pelo cliente. Mais flexível que a ROM para pequenas séries.
EPROM (Erasable Programmable ROM): Pode ser apagada expondo o chip à luz ultravioleta e reprogramada múltiplas vezes. Mais cara que a PROM, mas atualizável.
EEPROM (Electrically Erasable Programmable ROM): Pode ser apagada e regravada eletricamente, byte a byte, sem remover do circuito. É a mais flexível, mas também a mais cara e menos densa.
Memória Flash em Detalhe
Introduzida nos anos 80, a memória flash é uma tecnologia de semicondutor não volátil intermediária entre a EPROM e a EEPROM em custo e funcionalidade.
Operação: A memória flash utiliza um transistor com uma porta flutuante, isolada por uma camada de óxido. A aplicação de uma voltagem elevada aprisiona elétrons nessa porta, alterando o estado da célula para representar um '0'. Esse estado é mantido mesmo sem energia. Uma voltagem oposta remove os elétrons, retornando a célula ao estado '1'. O apagamento é feito eletricamente, em blocos, em uma única ação (um "flash"), o que é muito mais rápido que na EPROM.
Memória Flash NOR e NAND:
NOR Flash: As células são conectadas em paralelo. Isso permite acesso aleatório em nível de bit, similar a uma porta lógica NOR. É mais rápida para leitura e ideal para execução de código diretamente da memória (XIP - eXecute In Place), sendo tradicionalmente preferida para a memória interna de sistemas embarcados e armazenamento de firmware.
NAND Flash: As células são conectadas em série, similar a uma porta lógica NAND. Ela não permite acesso aleatório, e os dados são lidos e escritos em blocos (ou páginas). A NAND oferece maior densidade de bits, maior velocidade de escrita e menor custo por bit. É a tecnologia ideal para armazenamento secundário, dominando o mercado de SSDs, pendrives e cartões de memória.
Lógica e Encapsulamento do Chip
Organização do Chip: Os chips de memória contêm um array de células. Para economizar pinos, o endereço é frequentemente multiplexado: o endereço da linha (RAS - Row Address Select) e o da coluna (CAS - Column Address Select) são enviados pelos mesmos pinos de endereço em momentos diferentes.
Encapsulamento: O chip é montado em uma cápsula com pinos para conexão externa. Os pinos incluem linhas de endereço (A0, A1...), linhas de dados (D0, D1...), pinos de alimentação (Vcc, Vss), habilitação do chip (CE), e controles como habilitação de escrita (WE) e habilitação de saída (OE).
Memória Intercalada
A memória principal é composta por múltiplos chips de DRAM, que podem ser organizados em bancos. A memória intercalada armazena palavras consecutivas em bancos diferentes. Isso permite que o sistema acesse múltiplos bancos simultaneamente, aumentando a taxa de transferência de dados por um fator igual ao número de bancos.
6. Memória Externa
Esta seção detalha os principais dispositivos e sistemas de memória externa, começando pelo disco magnético, passando por arranjos de discos (RAID), discos de estado sólido (SSD) e, por fim, memória óptica.
Disco Magnético (HDD)
O disco magnético (Hard Disk Drive) é um prato circular (substrato) coberto por um material magnetizável. Modernamente, substratos de vidro são preferidos aos de alumínio por oferecerem maior uniformidade, menos defeitos e maior rigidez.
Leitura e Gravação Magnética:
Gravação: Uma corrente elétrica passa por uma bobina na cabeça de gravação, criando um campo magnético que magnetiza a superfície do disco em padrões que representam os dados.
Leitura: Sistemas mais antigos usavam a mesma cabeça para ler, explorando o fato de que um campo magnético em movimento induz corrente em uma bobina. Sistemas modernos usam uma cabeça de leitura separada com um sensor magnetorresistivo (MR), cuja resistência elétrica muda com a direção do campo magnético, permitindo maior densidade de dados e velocidade.
Organização e Formatação de Dados:
Trilhas e Setores: Os dados são organizados em anéis concêntricos chamados trilhas. Cada trilha é dividida em setores (geralmente de 512 bytes), que são as unidades de transferência de dados. Trilhas e setores são separados por lacunas (gaps) para evitar erros.
CAV vs. MZR: Para ler dados a uma taxa constante, os sistemas mais antigos usavam Velocidade Angular Constante (CAV), onde o disco gira a uma velocidade fixa. Isso limitava a capacidade, pois as trilhas externas, mais longas, armazenavam a mesma quantidade de dados que as internas. Sistemas modernos usam Gravação em Múltiplas Zonas (MZR), dividindo o disco em zonas. As zonas externas possuem mais setores por trilha, permitindo uma densidade de bits aproximadamente constante em todo o disco e, consequentemente, maior capacidade total.
Formatação: O disco é formatado com dados de controle, inacessíveis ao usuário, para localizar o início de cada trilha e setor. Cada setor possui um campo de ID com seu endereço (número da trilha e da cabeça) e um código de detecção de erros.
Características Físicas:
Cabeças: Podem ser fixas (uma por trilha) ou móveis (uma cabeça que se move radialmente). A maioria dos sistemas modernos usa cabeças móveis.
Pratos: Podem ser de face única ou dupla. Muitos drives utilizam múltiplos pratos empilhados verticalmente. O conjunto de trilhas na mesma posição em todos os pratos é chamado de cilindro.
Mecanismo da Cabeça: A cabeça pode entrar em contato com o disco (como em disquetes) ou "flutuar" sobre uma camada de ar. Os discos Winchester operam em um ambiente selado, permitindo que a cabeça aerodinâmica flutue muito perto da superfície (baixa altura de voo), o que possibilita maior densidade de dados.
Parâmetros de Desempenho: A transferência de E/S de um disco envolve vários passos:
Tempo de Busca (Seek Time): O tempo para mover o braço da cabeça de leitura/gravação até a trilha desejada.
Atraso Rotacional (Rotational Latency): O tempo de espera para que o setor desejado gire até a posição da cabeça.
Tempo de Acesso: A soma do tempo de busca e do atraso rotacional.
Tempo de Transferência: O tempo para ler ou escrever os dados do setor.
O tempo total para uma operação de leitura ou gravação é: T_total = T_busca + T_latência + T_transferência.
RAID (Redundant Array of Independent Disks)
RAID é uma tecnologia que combina múltiplos discos físicos em uma única unidade lógica para melhorar o desempenho, a confiabilidade ou ambos.
Características Comuns:
O sistema operacional vê o conjunto de discos como um único drive lógico.
Os dados são distribuídos pelos discos físicos usando striping (distribuição de dados em faixas).
A capacidade redundante é usada para armazenar informações de paridade, garantindo a recuperação de dados em caso de falha de um disco.
Níveis de RAID:
RAID 0 (Striping): Distribui os dados em faixas (strips) por todos os discos, sem redundância. Melhora o desempenho para grandes transferências de dados (alta taxa de transferência) e para múltiplas solicitações pequenas (alta taxa de solicitação), pois as operações podem ocorrer em paralelo. Não oferece tolerância a falhas.
RAID 1 (Mirroring): Duplica todos os dados em discos espelho. Oferece excelente confiabilidade, pois se um disco falhar, os dados estão disponíveis no outro. As solicitações de leitura podem ser atendidas pelo disco mais rápido, potencialmente dobrando a taxa de leitura. O custo é a principal desvantagem, pois requer o dobro do espaço de armazenamento.
RAID 2: Usa striping em nível de bit e dedica vários discos para armazenar um código de correção de erros (Hamming). É um exagero para os discos modernos e confiáveis, sendo raramente implementado.
RAID 3: Usa striping em nível de bit e dedica um único disco para armazenar informações de paridade. Todos os discos operam em paralelo para cada solicitação de E/S, oferecendo taxas de transferência muito altas para grandes arquivos, mas baixo desempenho em ambientes transacionais.
RAID 4: Usa striping em nível de bloco (strips grandes) e dedica um único disco para a paridade. Permite que solicitações de E/S independentes sejam atendidas em paralelo. No entanto, o disco de paridade pode se tornar um gargalo, pois toda operação de escrita precisa acessá-lo.
RAID 5: Similar ao RAID 4, mas distribui os blocos de paridade por todos os discos (striping de paridade). Isso elimina o gargalo do disco de paridade e oferece um bom equilíbrio entre desempenho, capacidade e confiabilidade. É uma das configurações mais comuns.
RAID 6: Similar ao RAID 5, mas utiliza dois cálculos de paridade independentes, distribuídos por todos os discos. Isso permite a recuperação de dados mesmo com a falha de dois discos simultaneamente, oferecendo altíssima disponibilidade. A desvantagem é uma "penalidade de escrita" maior, pois cada gravação precisa atualizar dois blocos de paridade.
Drives de Estado Sólido (SSD)
Um SSD é um dispositivo de memória não volátil que usa componentes semicondutores (tipicamente memória flash NAND) em vez de pratos magnéticos giratórios.
SSD vs. HDD:
Vantagens do SSD: Desempenho de E/S muito superior (IOPS), maior durabilidade (resistência a choques), menor consumo de energia, operação silenciosa e tempos de acesso e latência drasticamente menores.
Vantagem do HDD: Menor custo por gigabyte (embora a diferença esteja diminuindo).
Organização do SSD: Um SSD contém chips de memória flash NAND, um controlador (que gerencia o dispositivo e executa o firmware), um buffer/cache de RAM para otimizar o fluxo de dados, e lógica de correção de erros. A comunicação com o sistema hospedeiro é feita por interfaces como PCIe (interna) ou USB (externa).
Questões Práticas:
Degradação de Desempenho: A memória flash é lida em páginas (ex: 4 KB), mas deve ser apagada em blocos muito maiores (ex: 512 KB). Para modificar uma única página, o SSD precisa ler o bloco inteiro para um buffer, modificar a página e, em seguida, apagar e reescrever o bloco inteiro na memória flash. Com o tempo e a fragmentação de arquivos, uma única operação de escrita pode exigir a modificação de vários blocos, tornando o processo mais lento. Técnicas como over-provisioning (espaço extra) e o comando TRIM (que informa ao SSD quais blocos não estão mais em uso) ajudam a mitigar esse problema.
Vida Útil (Desgaste): As células de memória flash têm um número limitado de ciclos de escrita. Para prolongar a vida útil, os SSDs utilizam algoritmos de nivelamento de desgaste (wear-leveling), que distribuem as operações de escrita uniformemente por todas as células, além de técnicas de gerenciamento de blocos defeituosos.
Memória Óptica
A memória óptica utiliza um laser para ler e gravar dados em um disco.
CD (Compact Disc):
Tecnologia: Um laser de baixa potência lê uma espiral única de sulcos (pits) e pistas (lands) em um disco de policarbonato. A mudança na refletividade da luz entre um sulco e uma pista é interpretada como um 1 binário. Utiliza Velocidade Linear Constante (CLV), onde a velocidade de rotação do disco varia para manter a taxa de leitura de dados constante.
Tipos:
CD-ROM: Somente leitura, produzido em massa.
CD-R (Gravável): Permite uma única gravação.
CD-RW (Regravável): Usa um material de mudança de fase que pode ser gravado e apagado múltiplas vezes.
DVD (Digital Versatile Disc):
Maior Capacidade: Um DVD armazena significativamente mais dados que um CD (4,7 GB a 17 GB) devido a:
Bits mais compactos (sulcos e espiral mais próximos).
Uso de um laser de comprimento de onda menor.
Suporte a duas camadas (dual layer) de dados.
Capacidade de ser dupla face (double-sided).
Versões: Assim como os CDs, existem versões somente leitura (DVD-ROM), graváveis uma vez (DVD-R) e regraváveis (DVD-RW).
Discos Ópticos de Alta Definição (Blu-ray):
Tecnologia: Utiliza um laser azul-violeta com um comprimento de onda ainda menor que o do DVD. Isso permite sulcos e trilhas ainda menores, resultando em uma capacidade de armazenamento muito maior (tipicamente 25 GB por camada).
Formato: O Blu-ray (BD) venceu a concorrência com o HD DVD e se tornou o padrão para vídeo de alta definição, com versões somente leitura (BD-ROM), gravável (BD-R) e regravável (BD-RE).
7. Entrada/Saída (E/S)
Módulos de E/S: Atuam como uma interface entre a CPU/memória e os dispositivos periféricos, lidando com as enormes diferenças de velocidade e formato de dados. Suas funções incluem controle e temporização, comunicação, buffering de dados e detecção de erros.
Técnicas de E/S:
E/S Programada: A CPU tem controle total e espera ativamente ("polling") pela conclusão da operação de E/S. É simples, mas muito ineficiente.
E/S Controlada por Interrupção: O módulo de E/S notifica a CPU via um sinal de interrupção quando está pronto. Isso libera a CPU para executar outras tarefas enquanto espera, melhorando a eficiência.
Acesso Direto à Memória (DMA): Um módulo DMA dedicado gerencia a transferência de grandes blocos de dados entre a memória e a E/S sem qualquer envolvimento da CPU, exceto no início e no fim da transferência. É a técnica mais eficiente para transferências de grande volume.
8. Suporte do Sistema Operacional (SO)
Funções Principais: O SO atua como uma interface abstrata entre o hardware e o usuário/aplicações. Ele gerencia os recursos do computador (CPU, memória, E/S) para fornecer conveniência (uma plataforma mais fácil de usar) e eficiência (máxima utilização dos recursos).
Escalonamento: O SO gerencia a execução de múltiplos processos para maximizar o rendimento e a responsividade.
Escalonamento de longo prazo: Decide quais novos programas são admitidos no sistema para se tornarem processos.
Escalonamento de médio prazo: Lida com a troca de processos (swapping) entre a memória principal e o disco para gerenciar o grau de multiprogramação.
Escalonamento de curto prazo (dispatcher): Decide qual processo que está pronto irá executar na CPU a seguir.
Gerenciamento de Memória: O SO aloca a memória principal entre os vários processos em execução.
Particionamento, Paginação e Segmentação: São técnicas para dividir e alocar a memória.
Memória Virtual: É uma técnica poderosa, geralmente implementada com paginação por demanda, que desacopla o espaço de endereços lógico de um programa da memória física. Isso dá a cada processo a ilusão de ter um espaço de memória contínuo e muito maior do que a RAM física disponível. Permite a execução de programas maiores que a memória principal e aumenta o grau de multiprogramação.
Parte III e IV: Lógica, Aritmética e a Unidade Central de Processamento (CPU)
9-11. Sistemas Numéricos, Aritmética e Lógica Digital
Representação de Dados: A forma como os dados são representados em um computador é fundamental para seu funcionamento. Todas as informações são, em última análise, codificadas como sequências de bits.
Sistema Binário: O sistema fundamental de todos os computadores digitais. Opera na base 2, utilizando apenas os dígitos 0 e 1. O valor de um número binário é calculado como uma soma ponderada de potências de 2, onde cada posição de bit corresponde a uma potência. Por exemplo, o número binário 1011 equivale a (1 * 2^3) + (0 * 2^2) + (1 * 2^1) + (1 * 2^0) = 8 + 0 + 2 + 1 = 11 em decimal.
Notação Hexadecimal: Para facilitar a leitura e manipulação de longas sequências binárias por humanos, utiliza-se a notação hexadecimal (base 16). Cada dígito hexadecimal representa um grupo de 4 bits (um "nibble"). Os dígitos vão de 0 a 9 e de A a F (representando os valores de 10 a 15). A conversão é direta: 1011 0101 em binário se torna B5 em hexadecimal.
Complemento de Dois: É o método mais comum para representar inteiros com sinal. O bit mais à esquerda atua como bit de sinal (0 para positivo, 1 para negativo). Diferente da representação sinal-magnitude, ele possui uma única representação para o zero e simplifica as operações de adição e subtração, permitindo que a mesma lógica de circuito seja usada para números com e sem sinal. Para encontrar o negativo de um número, inverte-se todos os seus bits e soma-se 1.
Norma IEEE 754: Para representar números reais (com parte fracionária), utiliza-se a representação de ponto flutuante. A norma IEEE 754 é um padrão universal que define formatos para esses números, tipicamente divididos em três campos: um bit de sinal, um expoente (que determina a magnitude) e um significando ou mantissa (que determina a precisão). Este padrão garante que os cálculos de ponto flutuante sejam consistentes em diferentes plataformas de hardware e define como tratar valores especiais como infinito e NaN (Not a Number).
ALU (Unidade Lógica e Aritmética): Considerada o "cérebro" matemático da CPU. É um circuito digital complexo responsável por executar todas as operações aritméticas (adição, subtração, multiplicação, divisão) e lógicas (AND, OR, NOT, XOR) sobre os dados. A ALU recebe operandos dos registradores, realiza a operação solicitada pela unidade de controle e armazena o resultado em um registrador de destino. Além do resultado, a ALU atualiza flags de estado (como flags de zero, carry, overflow e sinal negativo), que são usadas por instruções de controle de fluxo para tomar decisões.
Lógica Digital: É o fundamento físico de todos os componentes do computador.
Portas Lógicas: São os blocos de construção mais básicos. São circuitos eletrônicos que implementam uma função booleana simples (AND, OR, NOT, etc.). Combinando essas portas, é possível construir qualquer circuito digital.
Circuitos Combinacionais: A saída desses circuitos depende exclusivamente das suas entradas atuais. Eles não possuem estado ou memória. Exemplos incluem somadores, que adicionam números binários, e decodificadores, que convertem um código de entrada em uma única linha de saída ativa. Uma ROM (Memória Somente de Leitura) pode ser vista como um grande circuito combinacional.
Circuitos Sequenciais: A saída desses circuitos depende das entradas atuais e do estado interno do circuito (seu histórico de entradas). Eles possuem memória. O elemento de memória mais fundamental é o flip-flop, que pode armazenar um único bit. Flip-flops são usados para construir componentes mais complexos como registradores (para armazenamento temporário de dados na CPU) e contadores (como o contador de programa, que aponta para a próxima instrução a ser executada).
12-16. Conjunto de Instruções, Estrutura do Processador e RISC
Instruções de Máquina: São os comandos que a CPU executa. Cada instrução possui um opcode (o que fazer) e operandos (sobre o que fazer). Os tipos de operação incluem transferência de dados, aritmética, lógica, e controle de fluxo (desvios, chamadas de procedimento).
Modos de Endereçamento: Por exemplo, no endereçamento por deslocamento, uma instrução como LOAD R1, 100(R2) calcularia o endereço efetivo somando o valor imediato 100 ao conteúdo do registrador R2. Isso é extremamente útil para acessar elementos de um array, onde R2 conteria o endereço base do array e 100 seria o deslocamento para um elemento específico. A escolha dos modos de endereçamento disponíveis impacta diretamente a flexibilidade e a eficiência do conjunto de instruções.
Estrutura do Processador: Composta pela ALU, unidade de controle e registradores. O ciclo de instrução (buscar, decodificar, executar) é realizado por uma sequência de micro-operações atômicas.
Pipeline: A eficiência do pipeline, no entanto, pode ser degradada por hazards (estruturais, de dados e de controle), que ocorrem quando uma instrução não pode prosseguir para o próximo estágio. Um hazard causa uma 'bolha' (pipeline bubble), que é um ciclo de clock perdido onde nenhuma instrução útil avança. O acúmulo de bolhas diminui o rendimento (throughput) do processador ao reduzir a taxa média de conclusão de instruções por ciclo (IPC). Portanto, o gerenciamento de hazards é uma das tarefas mais críticas no projeto de processadores de alto desempenho.
RISC vs. CISC: A filosofia CISC busca transferir a complexidade do software para o hardware, tentando criar instruções que se assemelhem a operações de linguagens de alto nível. Em contrapartida, a filosofia RISC transfere a complexidade do hardware para o software (o compilador). Ao fornecer um conjunto de instruções simples e regular, o hardware pode ser mais rápido e eficiente, enquanto o compilador assume a tarefa mais complexa de combinar essas instruções primitivas para realizar operações de alto nível. Essa troca é um dos debates mais fundamentais na história da arquitetura de computadores, e os processadores modernos frequentemente combinam elementos de ambas as filosofias.
Processadores Superescalares: É uma evolução da arquitetura de pipeline. Um processador superescalar possui múltiplas unidades de execução em paralelo, permitindo que várias instruções sejam executadas completamente no mesmo ciclo de clock, desde que não haja dependências entre elas.
Parte V: Processamento Paralelo
17. Organizações Paralelas de Processadores
Taxonomia de Flynn: Ainda a maneira mais comum de categorizar sistemas com capacidade de processamento paralelo. Flynn propôs as seguintes categorias de sistemas computacionais:
Instrução única, único fluxo de dado (SISD — do inglês, Single Instruction, Single Data): um processador único executa uma única sequência de instruções para operar os dados armazenados em uma única memória. Uniprocessadores enquadram-se nessa categoria.
Instrução única, múltiplos fluxos de dados (SIMD — do inglês, Single Instruction, Multiple Data): uma única instrução de máquina controla a execução simultânea de uma série de elementos de processamento em operações básicas. Cada elemento de processamento possui uma memória de dados associada, então cada instrução é executada em um conjunto diferente de dados por processadores diferentes. Processadores de vetores e matrizes se enquadram nessa categoria.
Múltiplas instruções, único fluxo de dado (MISD — do inglês, Multiple Instruction, Single Data): uma sequência de dados é transmitida para um conjunto de processadores, em que cada um executa uma sequência de instruções diferente. Essa estrutura não é implementada comercialmente.
Múltiplas instruções, múltiplos fluxos de dados (MIMD — do inglês, Multiple Instruction, Multiple Data): um conjunto de processadores que executam sequências de instruções diferentes simultaneamente em diferentes conjuntos de dados. SMPs, clusters e sistemas NUMA enquadram-se nessa categoria.
Multiprocessadores Simétricos (SMP):
Arquitetura: Múltiplos processadores idênticos compartilham uma única memória principal e os mesmos dispositivos de E/S. O acesso à memória é uniforme (UMA - Uniform Memory Access), significando que qualquer processador pode acessar qualquer parte da memória com a mesma latência. A interconexão é tipicamente um barramento compartilhado.
Vantagens: Facilidade de gerenciamento, pois o sistema operacional enxerga um único espaço de memória e pode escalonar tarefas para qualquer processador.
Desafio: O barramento compartilhado pode se tornar um gargalo de desempenho e coerência de cache à medida que o número de processadores aumenta, limitando a escalabilidade.
Multithreading e Chips Multiprocessadores:
Multithreading: É uma técnica que permite que um único core de processador execute múltiplas threads (fluxos de execução de software) de forma concorrente. Ele mascara a latência (ex: esperas por memória) alternando entre threads. As abordagens incluem SMT (Simultaneous Multithreading), onde instruções de múltiplas threads são executadas em paralelo no mesmo ciclo de clock em um core superescalar.
Chips Multiprocessadores (Multicore): É a prática de integrar múltiplos cores de processador em um único chip. Isso se tornou a abordagem dominante para aumentar o desempenho, pois supera os limites de frequência e consumo de energia de um único core.
Clusters:
Arquitetura: São grupos de computadores completos e independentes (nós) interconectados por uma rede de alta velocidade. Cada nó tem sua própria memória privada (memória distribuída).
Vantagens: Altamente escalável (pode-se adicionar mais nós) e oferece alta disponibilidade, pois a falha de um nó não derruba o sistema inteiro (requer software para failover).
Desafio: A programação é mais complexa, pois a comunicação entre os nós deve ser explícita (troca de mensagens), e a latência de comunicação é maior do que em sistemas de memória compartilhada.
18. Computadores Multicore
Questões de Desempenho:
Hardware: A mudança para multicore foi impulsionada pela "muralha da potência" (power wall), o ponto em que aumentar a frequência de clock de um único core se tornou insustentável devido ao consumo de energia e à dissipação de calor. A Lei de Pollack sugere que o ganho de desempenho de um core é proporcional à raiz quadrada do aumento de sua complexidade, enquanto adicionar mais cores pode, teoricamente, levar a um ganho de desempenho quase linear.
Software: O principal desafio é que o software precisa ser projetado para ser paralelo para aproveitar os múltiplos cores. A Lei de Amdahl mostra que a parte sequencial de um programa limita o ganho de desempenho. Aplicações multithread, aplicações com múltiplos processos e aplicações Java são exemplos de softwares que se beneficiam de arquiteturas multicore.
Organização Multicore Heterogênea:
Conceito: Em vez de usar apenas cores idênticos (homogêneos), esta abordagem combina diferentes tipos de cores em um único chip para otimizar para diferentes tipos de tarefas.
Exemplos:
CPU + GPU: A integração de cores de CPU de uso geral com cores de GPU massivamente paralelos no mesmo chip.
big.LITTLE (ARM): Combina "big" cores de alto desempenho com "LITTLE" cores de baixo consumo de energia. O sistema operacional pode mover tarefas entre os diferentes tipos de cores para balancear desempenho e eficiência energética, ideal para dispositivos móveis.
19. Unidades de Processamento Gráfico de Uso Geral (GPGPU)
Noções Básicas sobre a CUDA: A CUDA (Compute Unified Device Architecture) é a plataforma de programação paralela da NVIDIA que expõe a arquitetura da GPU para computação de uso geral. O programador escreve kernels (funções) que são executados por um grande número de threads na GPU. Esses threads são organizados em uma hierarquia de blocos e redes (grids), o que permite ao programador mapear o paralelismo da aplicação para o hardware massivamente paralelo da GPU.
GPU versus CPU:
CPUs são otimizadas para baixa latência e execução sequencial rápida de tarefas complexas e diversas. Elas possuem unidades de controle sofisticadas e grandes caches.
GPUs são otimizadas para alto rendimento (throughput) e paralelismo de dados. Elas possuem milhares de cores simples que executam a mesma instrução em diferentes dados (arquitetura SIMD), ideal para gráficos, simulações e machine learning.
Visão Geral da Arquitetura de uma GPU: Uma GPU típica é composta por vários Multiprocessadores de Streaming (SMs). Cada SM contém um conjunto de cores de processamento, uma memória compartilhada/cache L1 e um conjunto de registradores. Um escalonador de warp dentro do SM gerencia a execução de grupos de threads (chamados warps).
Quando usar uma GPU como um Coprocessador: GPUs são ideais para acelerar partes de um programa que são: 1) computacionalmente intensivas e 2) massivamente paralelas (data-parallel), ou seja, a mesma operação pode ser aplicada a muitos elementos de dados de forma independente. O ganho de desempenho deve compensar a sobrecarga de transferir dados entre a memória da CPU e a memória da GPU.
Parte VI: A Unidade de Controle
20. Operação da Unidade de Controle
Micro-operações: A execução de uma instrução de máquina complexa (como ADD [mem], R1) é decomposta pela unidade de controle em uma sequência de passos fundamentais e atômicos chamados micro-operações. Exemplos incluem transferir dados entre registradores, ler da memória para um registrador, ou ativar uma função específica da ALU. A unidade de controle é responsável por orquestrar a sequência correta dessas micro-operações para realizar a função da instrução original.
Função: A unidade de controle gera os sinais de controle necessários para executar cada micro-operação. Ela recebe como entrada o registrador de instrução (para saber o que fazer), o clock (para temporização) e os flags de estado da ALU (para tomar decisões), e gera como saída sinais de controle internos (para a ALU e registradores) e externos (para o barramento do sistema).
21. Controle Microprogramado
Conceito: É uma alternativa à implementação da unidade de controle diretamente em hardware com portas lógicas ("hardwired"). No controle microprogramado, a lógica de controle é armazenada em uma memória especial (a memória de controle) na forma de um programa chamado microprograma ou firmware.
Microinstruções: Cada linha desse microprograma é uma microinstrução, que especifica os sinais de controle a serem gerados e como determinar o endereço da próxima microinstrução a ser executada.
Horizontal vs. Vertical: As microinstruções podem ser horizontais (largas, com um bit por sinal de controle, oferecendo alto paralelismo) ou verticais (estreitas, com campos codificados que precisam ser decodificados para gerar os sinais, resultando em menos paralelismo, mas um formato mais compacto).
Vantagens e Desvantagens: A flexibilidade do controle microprogramado permite que os fabricantes corrijam bugs no projeto do processador ou até mesmo adicionem novas instruções de máquina apenas atualizando o firmware, sem a necessidade de uma custosa reprojetação do hardware. Por outro lado, a implementação em hardware é significativamente mais rápida porque os sinais de controle são gerados diretamente pela lógica de portas, eliminando a sobrecarga de buscar e decodificar microinstruções da memória de controle.
