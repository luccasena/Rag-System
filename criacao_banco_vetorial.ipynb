{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fbade71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            contents = file.read()\n",
    "        return contents\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found:\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc92c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferir_metadados(texto, filename):\n",
    "    match = re.search(r\"(Parte\\s+[IVXL]+:.*?)\\n\", texto)\n",
    "    return {\n",
    "        \"nome do arquivo\": filename,\n",
    "        \"se√ß√£o\": match.group(1) if match else \"Desconhecida\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162893f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucca\\AppData\\Local\\Temp\\ipykernel_34608\\4038600018.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  client = Ollama(model=\"llama3:8b\")\n",
      "C:\\Users\\lucca\\AppData\\Local\\Temp\\ipykernel_34608\\4038600018.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "client = Ollama(model=\"llama3:8b\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-large\",\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    ")\n",
    "\n",
    "chunk_size = 1300 \n",
    "chunk_overlap = 200\n",
    "file_path = \"documents/OAC-Resumo.txt\"\n",
    "persist_directory = \"chroma-oac\"\n",
    "\n",
    "criar_db = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b39d1a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando Banco Vetorial\n",
      "Banco de vetores carregado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "if criar_db:\n",
    "    print(\"Criando Banco Vetorial\")\n",
    "\n",
    "    # Abrindo o arquivo\n",
    "    texto = open_file(file_path)\n",
    "    filename = os.path.basename(file_path)\n",
    "    metadatas = [{\"nome do arquivo\": filename}]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "                                        chunk_size=chunk_size,\n",
    "                                        chunk_overlap= chunk_overlap,\n",
    "                                        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "                                        )\n",
    "\n",
    "\n",
    "    documentos = []\n",
    "    for chunk in text_splitter.split_text(texto):\n",
    "        metadados = inferir_metadados(chunk, filename)\n",
    "        documentos.append(Document(page_content=chunk, metadata=metadados))\n",
    "\n",
    "    vectorstore = Chroma.from_documents(documents=documentos, embedding=embedding, persist_directory=persist_directory)\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Carregando Banco Vetorial\")\n",
    "    try:\n",
    "        vectorstore = Chroma(embedding_function=embedding, persist_directory=persist_directory)\n",
    "        print(\"Banco de vetores carregado com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar banco de vetores: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae0f2450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Score: 0.3450\n",
      "Estrutura do Processador: Composta pela ALU, unidade de controle e registradores. O ciclo de instru√ß√£o (buscar, decodificar, executar) √© realizado por uma sequ√™ncia de micro-opera√ß√µes at√¥micas.\n",
      "Pipeline: A efici√™ncia do pipeline, no entanto, pode ser degradada por hazards (estruturais, de dados e de\n",
      "\n",
      "üìÑ Score: 0.3677\n",
      "2. Quest√µes de Desempenho\n",
      "Projeto para Desempenho: A otimiza√ß√£o do desempenho envolve um esfor√ßo cont√≠nuo para aumentar a taxa de execu√ß√£o de instru√ß√µes. As principais t√©cnicas incluem:\n",
      "Pipeline: Sobrep√µe a execu√ß√£o de m√∫ltiplas instru√ß√µes em diferentes est√°gios, como uma linha de montagem.\n",
      "Previs√£o\n",
      "\n",
      "üìÑ Score: 0.4039\n",
      "RISC vs. CISC: A filosofia CISC busca transferir a complexidade do software para o hardware, tentando criar instru√ß√µes que se assemelhem a opera√ß√µes de linguagens de alto n√≠vel. Em contrapartida, a filosofia RISC transfere a complexidade do hardware para o software (o compilador). Ao fornecer um con\n",
      "\n",
      "üìÑ Score: 0.4086\n",
      "Lei de Little: Um princ√≠pio fundamental da teoria de filas, muito √∫til para an√°lise de desempenho de sistemas. Ela relaciona o n√∫mero m√©dio de itens em um sistema (L), a taxa m√©dia de chegada de itens (Œª) e o tempo m√©dio que um item passa no sistema (W) atrav√©s da simples f√≥rmula: L = ŒªW.\n",
      "Benchmarks\n",
      "\n",
      "üìÑ Score: 0.4164\n",
      "Multithreading e Chips Multiprocessadores:\n",
      "Multithreading: √â uma t√©cnica que permite que um √∫nico core de processador execute m√∫ltiplas threads (fluxos de execu√ß√£o de software) de forma concorrente. Ele mascara a lat√™ncia (ex: esperas por mem√≥ria) alternando entre threads. As abordagens incluem SMT \n",
      "\n",
      "üìÑ Score: 0.4181\n",
      "Taxonomia de Flynn: Ainda a maneira mais comum de categorizar sistemas com capacidade de processamento paralelo. Flynn prop√¥s as seguintes categorias de sistemas computacionais:\n",
      "Instru√ß√£o √∫nica, √∫nico fluxo de dado (SISD ‚Äî do ingl√™s, Single Instruction, Single Data): um processador √∫nico executa uma\n"
     ]
    }
   ],
   "source": [
    "question = \"Qual a defini√ß√£o de um pipeline?\"\n",
    "\n",
    "retriever  = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "docs_scores = vectorstore.similarity_search_with_score(question, k=6)\n",
    "for doc, score in docs_scores:\n",
    "    print(f\"\\nüìÑ Score: {score:.4f}\")\n",
    "    print(doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f97c1af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: **Resposta:**\n",
      "**Taxonomia de Flynn e categorias de sistemas computacionais**\n",
      "\n",
      "A Taxonomia de Flynn √© uma forma comum de categorizar sistemas com capacidade de processamento paralelo, proposta por Michael J. Flynn em 1972. A taxonomia define quatro categorias de sistemas computacionais baseadas na estrutura do sistema e na natureza da instru√ß√£o:\n",
      "\n",
      "* **SISD (Single Instruction, Single Data)**: um processador √∫nico executa uma √∫nica sequ√™ncia de instru√ß√µes para operar os dados armazenados em uma √∫nica mem√≥ria. Uniprocessadores enquadram-se nessa categoria.\n",
      "* **SIMD (Single Instruction, Multiple Data)**: uma √∫nica instru√ß√£o de m√°quina controla a execu√ß√£o simult√¢nea de uma s√©rie de elementos de processamento em opera√ß√µes b√°sicas. Cada elemento de processamento possui uma mem√≥ria de dados associada, ent√£o cada instru√ß√£o √© executada em um conjunto diferente de dados por processadores diferentes. Processadores de vetores e matrizes se enquadram nessa categoria.\n",
      "* **MISD (Multiple Instruction, Single Data)**: uma sequ√™ncia de dados √© transmitida para um conjunto de processadores, em que cada um executa uma sequ√™ncia de instru√ß√µes diferente. Essa estrutura n√£o √© implementada comercialmente.\n",
      "* **MIMD (Multiple Instruction, Multiple Data)**: um conjunto de processadores que executam sequ√™ncias de instru√ß√µes diferentes simultaneamente em diferentes conjuntos de dados. SMPs, clusters e sistemas NUMA enquadram-se nessa categoria.\n",
      "\n",
      "Essas categorias ajudam a entender como os sistemas computacionais lidam com o processamento paralelo e podem ser √∫teis para projetar e implementar sistemas eficientes.\n",
      "\n",
      "**Avalia√ß√£o percentual:** 95%\n",
      "\n",
      "**Explica√ß√£o:** A resposta √© baseada nos trechos do texto que descrevem a Taxonomia de Flynn e as categorias de sistemas computacionais. Os trechos mais coerentes com a resposta s√£o os que definem cada categoria e sua caracter√≠stica mais importante (SISD: um processador √∫nico; SIMD: execu√ß√£o simult√¢nea; MISD: sequ√™ncia de dados para processadores diferentes; MIMD: conjuntos de processadores executando sequ√™ncias de instru√ß√µes diferentes).\n"
     ]
    }
   ],
   "source": [
    "def rag_consultas(question):\n",
    "\n",
    "    context = retriever.get_relevant_documents(question)\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "\n",
    "        template=\"\"\"\n",
    "            \n",
    "            Voc√™ √© um assistente especialista em Organiza√ß√£o e Arquitetura de Computadores com o objetivo de auxiliar estudantes da disciplina a entenderem o conte√∫do.\n",
    "\n",
    "            Baseie sua resposta **exclusivamente** no texto abaixo. Sua resposta deve ser gerada em **formato Markdown**, clara, objetiva e adequada para **estudantes iniciantes**. Para quest√µes t√©cnicass, caso a pergunta solicitada n√£o esteja em conformidade com o texto sugerido, expor o texto que foi enviado para refer√™ncia.\n",
    "\n",
    "            Inclua sempre ao final:\n",
    "            - Uma **avalia√ß√£o percentual** de quanto o texto de apoio foi essencial para responder √† pergunta.\n",
    "            - Uma explica√ß√£o breve indicando **quais trechos do texto** foram mais coerentes com a resposta.\n",
    "\n",
    "            Responda sempre em **portugu√™s do Brasil**, usando linguagem acess√≠vel e, quando necess√°rio, explique os termos t√©cnicos com analogias ou exemplos simples.\n",
    "\n",
    "            ---\n",
    "\n",
    "            ### üìò Texto de apoio:\n",
    "            {context}\n",
    "\n",
    "            ---\n",
    "\n",
    "            ### ‚ùì Pergunta:\n",
    "            {question}\n",
    "\n",
    "            ---\n",
    "\n",
    "            ### ‚úÖ Resposta (em Markdown):\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "    )\n",
    "\n",
    "    prompt = prompt_template.format(context=context, question=question)\n",
    "\n",
    "    \"\"\"\n",
    "    result = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    result = client.invoke(prompt)\n",
    "\n",
    "    return result\n",
    "    \n",
    "    \n",
    "resposta = rag_consultas(question)\n",
    "\n",
    "print(\"Resposta:\", resposta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
